{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Agent for Resource Optimization\n",
        "\n",
        "This notebook demonstrates a reinforcement learning (RL) agent designed for optimizing resources in a smart environment.\n",
        "We cover:\n",
        "- Environment setup\n",
        "- Defining the RL agent\n",
        "- Training the agent using Q-learning\n",
        "- Evaluation and example policy\n"
      ],
      "metadata": {
        "id": "ZkezvH9HAYLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy matplotlib gym\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n"
      ],
      "metadata": {
        "id": "sAYuDKYSAcBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResourceEnv:\n",
        "    def __init__(self, max_resources=5):\n",
        "        self.max_resources = max_resources\n",
        "        self.state = self.max_resources // 2  # initial resources\n",
        "        self.action_space = [0,1]  # 0: consume, 1: replenish\n",
        "        self.n_actions = len(self.action_space)\n",
        "        self.n_states = self.max_resources + 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.max_resources // 2\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # consume\n",
        "            self.state = max(0, self.state - 1)\n",
        "        else:  # replenish\n",
        "            self.state = min(self.max_resources, self.state + 1)\n",
        "        # reward: maintain mid-level resources\n",
        "        reward = -abs(self.state - self.max_resources//2)\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f'Resources level: {self.state}')\n"
      ],
      "metadata": {
        "id": "NgKgVKzCAcxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.n_actions)\n",
        "        return np.argmax(self.Q[state])\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        best_next = np.max(self.Q[next_state])\n",
        "        self.Q[state, action] += self.alpha * (reward + self.gamma*best_next - self.Q[state, action])\n"
      ],
      "metadata": {
        "id": "Wb5GlxSoAexp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = ResourceEnv()\n",
        "agent = QLearningAgent(env.n_states, env.n_actions)\n",
        "\n",
        "episodes = 1000\n",
        "rewards = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in range(10):\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.update(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Training Progress')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sPNpFAz8Agv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "print(\"Learned policy (0: consume, 1: replenish) over 10 steps:\")\n",
        "for t in range(10):\n",
        "    action = np.argmax(agent.Q[state])\n",
        "    print(f'Step {t+1}, State {state}, Action {action}')\n",
        "    state, _, _, _ = env.step(action)\n"
      ],
      "metadata": {
        "id": "nKW3sTSJAmMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates a basic RL agent using Q-learning:\n",
        "- Custom environment simulating resource management\n",
        "- Q-learning update and epsilon-greedy policy\n",
        "- Training progress visualization\n",
        "- Evaluation of learned policy\n",
        "\n",
        "The framework is ready for extension to more complex environments and larger state-action spaces.\n"
      ],
      "metadata": {
        "id": "BDejII4zAq84"
      }
    }
  ]
}
